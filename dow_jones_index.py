# -*- coding: utf-8 -*-
"""Dow_Jones_Index.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Vf0oba7Nql9jiU_boTk-I3XpF0p_tkFP

# **Amira Alaa Al-Maghawry Mohamed**

# **Dow Jones Index **
"""

import numpy as np 
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import train_test_split
import pandas as pd 
import seaborn as sns
import matplotlib.pyplot as plt 
#Scikit-learn models
from sklearn import linear_model
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.metrics import confusion_matrix
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVR
from sklearn.tree import DecisionTreeClassifier
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import AdaBoostClassifier
from sklearn.neighbors import KNeighborsRegressor
from sklearn.naive_bayes import GaussianNB 
# Scikit-learn metrics
from sklearn.metrics import accuracy_score
from sklearn.metrics import r2_score
from sklearn import metrics

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
filelist =['/content/AAPL.csv',"/content/DWDP.csv","/content/NKE.csv","/content/HD.csv","/content/GS.csv","/content/AXP.csv","/content/BA.csv","/content/CAT.csv","/content/CSCO.csv","/content/CVX.csv","/content/DIS.csv"]
# Load data
data = pd.read_csv('/content/AAPL.csv')

cols = ['date','open', 'high', 'low', 'close', 'volume','change']
data = data[cols].copy()

#************************************
df_stocks = pd.DataFrame()

for i in filelist:
    tmp = pd.read_csv(i)
    tmp['symbol'] = i.split('/')[-1].split('.')[0]
    df_stocks = df_stocks.append(tmp)

cols = ['date','open', 'high', 'low', 'close', 'volume', 'symbol']
df_stocks = df_stocks[cols].copy()
df_stocks['date'] = pd.to_datetime(df_stocks['date'])

# create new dataframe with just closing price for each stock
df2 = df_stocks.pivot(index='date', columns='symbol', values='close')

data

df_stocks

df2

missing={"missing":data.isnull().sum()," % of missing":round(((data.isnull().sum()/data.shape[0])*100),2)}
pd.DataFrame(missing)

data.duplicated().sum()

data.info()

data['date']=pd.to_datetime(data['date'])

data=data.set_index("date")

data.head(10)

data.tail(10)

data.describe()

data['open'].plot(figsize=(20,10))

data[['open','high','low','close']].plot(figsize=(20,10))

data['volume'].plot(figsize=(20,10))

plt.figure(figsize=(15,7))
top = plt.subplot2grid((4,4), (0, 0), rowspan=3, colspan=4)
bottom = plt.subplot2grid((4,4), (3,0), rowspan=1, colspan=4)
top.plot(data.index, data['close']) 
bottom.bar(data.index, data['volume']) 
 
# set the labels
top.axes.get_xaxis().set_visible(False)
top.set_ylabel('Closing Price')
bottom.set_ylabel('Volume');

from pylab import rcParams
import statsmodels.api as sm
rcParams['figure.figsize'] = 18, 20
decomposition = sm.tsa.seasonal_decompose(data['close'], model='multiplicative', freq=1)
fig = decomposition.plot()
plt.show()

rcParams['figure.figsize'] = 18, 20
decomposition = sm.tsa.seasonal_decompose(data['open'], model='multiplicative', freq=1)
fig = decomposition.plot()
plt.show()

# simple moving averages
sma5 = data['open'].rolling(10).mean() #5 days
sma200 = data['open'].rolling(200).mean() #100 days
 
AAPL_sma = pd.DataFrame({'AAPL': data['close'], 'SMA 10': sma5, 'SMA 200': sma200})
AAPL_sma.plot(figsize=(15, 7), legend=True, title='AAPL');

df2.plot(figsize=(15,8))
plt.ylabel('Price');

# Compute the correlation matrix
corr = df2.corr()

# Generate a mask for the upper triangle
mask = np.zeros_like(corr, dtype=np.bool)
mask[np.triu_indices_from(mask)] = True

# Set up the matplotlib figure
f, ax = plt.subplots(figsize=(10, 10))

# Generate a custom diverging colormap
cmap = sns.diverging_palette(220, 10, as_cmap=True)

# Draw the heatmap with the mask and correct aspect ratio
sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,
            square=True, linewidths=.5, cbar_kws={"shrink": .5});

data.shape

data.columns

lags = pd.DataFrame()
for i in range(10,0,-1):
    lags['t-'+str(i)] = data["open"].shift(i)
    lags['t'] = data['open'].values
lags = lags[10:]

lags

array = lags.values
X= array[:,0:-1]
y = array[:,-1]

from sklearn.ensemble import RandomForestRegressor

model = RandomForestRegressor(n_estimators=1200, random_state=1,min_samples_leaf=1,min_samples_split=2,max_features='auto',max_depth=15)
model.fit(X, y)

import plotly.graph_objects as go
names = lags.columns
fig = go.Figure(data=[go.Bar(
            x=lags.columns, y=model.feature_importances_,
            text=model.feature_importances_,
            textposition='auto',)])

fig.show()

from sklearn.feature_selection import RFE
rfe = RFE(RandomForestRegressor(n_estimators=1200, random_state=1,min_samples_leaf=1,min_samples_split=2,max_features='auto',max_depth=15), 4)
fit = rfe.fit(X, y)
names = lags.columns
columns=[]
for i in range(len(fit.support_)):
    if fit.support_[i]:
        columns.append(names[i])

print("Columns with predictive power:", columns )

from statsmodels.tsa.seasonal import seasonal_decompose
import plotly.graph_objects as go
result = seasonal_decompose(data, model='additive', freq=1)
fig = go.Figure()
fig.add_trace(go.Scatter(x=result.seasonal.index, y=result.seasonal.values, mode='lines', name='Seasonal'))

columns=data.columns
data_mean=pd.DataFrame(columns=columns)
for col in columns:
    data_mean[col] = data[col].rolling(window = 4).mean()
data_mean = data_mean.dropna()

import plotly.graph_objects as go
fig = go.Figure()
fig.add_trace(go.Scatter(x=data.index, y=data['open'], mode='lines', name='open'))
fig.add_trace(go.Scatter(x=data.index, y=data_mean['open'], mode='lines', name='open- Rolling Mean'))

df_forecasting=pd.DataFrame(data["open"])
df_forecasting["open_diff"] = df_forecasting["open"].diff()
for i in range(4,0,-1):
    df_forecasting['t-'+str(i)] = df_forecasting["open"].shift(i)
df_forecasting=df_forecasting.dropna()
df_forecasting["open_rolling"] = df_forecasting["open"].rolling(window = 4).mean()
df_forecasting= df_forecasting.dropna()

df_forecasting

from sklearn.metrics import mean_absolute_error
from sklearn.ensemble import RandomForestRegressor
x=df_forecasting.iloc[:,0:5]
y=df_forecasting.iloc[:,5]
x_train, x_valid = x[0:943], x[944:]
y_train, y_valid = y[0:943], y[944:]

from sklearn.model_selection import TimeSeriesSplit, cross_val_score, GridSearchCV

time_split = TimeSeriesSplit(n_splits=10)

from sklearn.model_selection import RandomizedSearchCV
from sklearn.metrics import mean_squared_error

random_grid = {'n_estimators': [1200],
               'max_features': ['auto'],
               'max_depth': [15],
               'min_samples_split': [2],
               'min_samples_leaf': [1]}

rf = RandomForestRegressor(random_state=1)
grid_cv_dtm = GridSearchCV(rf, random_grid, cv=time_split)
grid_cv_dtm.fit(x_train,y_train)

df = pd.DataFrame(data=grid_cv_dtm.cv_results_)
df.head()

print("R-Squared::{}".format(grid_cv_dtm.best_score_))
print("Best Hyperparameters::\n{}".format(grid_cv_dtm.best_params_))

# Checking the training model scores
r2_scores = cross_val_score(grid_cv_dtm.best_estimator_, x_train,y_train, cv=time_split)
mse_scores = cross_val_score(grid_cv_dtm.best_estimator_, x_train,y_train, cv=time_split,scoring='neg_mean_squared_error')
print("avg R-squared::{:.3f}".format(np.mean(r2_scores)))
print("MSE::{:.3f}".format(abs(np.mean(mse_scores))))

# Create the random grid
random_grid = {"n_neighbors":[50,60,70,80],
}
rf = KNeighborsRegressor()
grid_cv_dtm = GridSearchCV(rf, random_grid,cv=time_split)
grid_cv_dtm.fit(x_train,y_train)

df = pd.DataFrame(data=grid_cv_dtm.cv_results_)
df.head()

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
import numpy as np
import matplotlib
from keras.models import Sequential, load_model
from keras.layers.core import Dense,Flatten, Dropout, Activation
from keras.utils import np_utils
# %matplotlib inline

print("X_train shape", x_train.shape)
print("y_train shape", y_train.shape)
print("X_test shape", x_valid.shape)
print("y_test shape", y_valid.shape)

from tensorflow.keras.layers import BatchNormalization

model =Sequential()

model.add(Dense(500, input_shape=(5,)))
model.add(BatchNormalization())


model.add(Dense(500))
model.add(BatchNormalization())


model.add(Dense(16))
model.add(BatchNormalization())


model.add(Dense(307))

model.compile(loss="binary_crossentropy",
              optimizer="adam",
              metrics=['accuracy'])

model.summary()

# training the model and saving metrics in history
history = model.fit(x_train,y_train,
          batch_size=128, epochs=20,
          verbose=2,
          validation_data=(x_valid,y_valid))

# plotting the metrics
fig = plt.figure()
plt.subplot(2,1,1)
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='lower right')

plt.subplot(2,1,2)
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper right')

plt.tight_layout()